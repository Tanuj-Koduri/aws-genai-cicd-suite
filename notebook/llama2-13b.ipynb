{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!%pip install sagemaker --upgrade  --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import boto3\n",
    "import sagemaker\n",
    "from sagemaker import Model, image_uris, serializers, deserializers\n",
    "\n",
    "role = sagemaker.get_execution_role()  # execution role for the endpoint\n",
    "sess = sagemaker.session.Session()  # sagemaker session for interacting with different AWS APIs\n",
    "region = sess._region_name  # region name of the current SageMaker Studio environment\n",
    "account_id = sess.account_id()  # account_id of the current SageMaker Studio environment\n",
    "sm_client = boto3.client(\"sagemaker\")\n",
    "smr_client = boto3.client(\"sagemaker-runtime\")\n",
    "print(f\"role: {role}\\nsess: {sess}\\nregion: {region}\\naccount_id: {account_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_trtllm = {\"HUGGINGFACE_HUB_CACHE\": \"/tmp\",\n",
    "              \"TRANSFORMERS_CACHE\": \"/tmp\",\n",
    "              \"SERVING_LOAD_MODELS\": \"test::MPI=/opt/ml/model\",\n",
    "              \"OPTION_MODEL_ID\": \"codellama/CodeLlama-13b-hf\",\n",
    "              \"OPTION_TRUST_REMOTE_CODE\": \"true\",\n",
    "              \"OPTION_TENSOR_PARALLEL_DEGREE\": \"max\",\n",
    "              \"OPTION_ROLLING_BATCH\": \"trtllm\",\n",
    "              \"OPTION_MAX_ROLLING_BATCH_SIZE\": \"32\",\n",
    "              \"OPTION_DTYPE\":\"fp16\"\n",
    "             }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trtllm_image_uri = image_uris.retrieve(\n",
    "    framework=\"djl-tensorrtllm\",\n",
    "    region=sess.boto_session.region_name,\n",
    "    version=\"0.26.0\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_lmidist = {\"HUGGINGFACE_HUB_CACHE\": \"/tmp\",\n",
    "               \"TRANSFORMERS_CACHE\": \"/tmp\",\n",
    "               \"SERVING_LOAD_MODELS\": \"test::MPI=/opt/ml/model\",\n",
    "               \"OPTION_MODEL_ID\": \"codellama/CodeLlama-13b-hf\",\n",
    "               \"OPTION_TRUST_REMOTE_CODE\": \"true\",\n",
    "               \"OPTION_TENSOR_PARALLEL_DEGREE\": \"max\",\n",
    "               \"OPTION_ROLLING_BATCH\": \"lmi-dist\",\n",
    "               \"OPTION_MAX_ROLLING_BATCH_SIZE\": \"32\",\n",
    "               \"OPTION_DTYPE\":\"fp16\"\n",
    "              }\n",
    "\n",
    "deepspeed_image_uri = image_uris.retrieve(\n",
    "    framework=\"djl-deepspeed\", \n",
    "    region=sess.boto_session.region_name, \n",
    "    version=\"0.26.0\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# - Select the appropriate environment variable which will tune the deployment server.\n",
    "env = env_trtllm\n",
    "#env = env_lmidist # use this when generating tokens > 1024  \n",
    "\n",
    "# - now we select the appropriate container \n",
    "#inference_image_uri = deepspeed_image_uri # use this when generating tokens > 1024 \n",
    "inference_image_uri = trtllm_image_uri\n",
    "\n",
    "print(f\"Environment variables are ---- > {env}\")\n",
    "print(f\"Image going to be used is ---- > {inference_image_uri}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = sagemaker.utils.name_from_base(\"lmi-codellama-7b\")\n",
    "print(model_name)\n",
    "\n",
    "create_model_response = sm_client.create_model(\n",
    "    ModelName=model_name,\n",
    "    ExecutionRoleArn=role,\n",
    "    PrimaryContainer={\n",
    "        \"Image\": inference_image_uri,\n",
    "        \"Environment\": env,\n",
    "    }\n",
    ")\n",
    "model_arn = create_model_response[\"ModelArn\"]\n",
    "\n",
    "print(f\"Created Model: {model_arn}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint_config_name = f\"{model_name}-config\"\n",
    "endpoint_name = f\"{model_name}-endpoint\"\n",
    "\n",
    "endpoint_config_response = sm_client.create_endpoint_config(\n",
    "    EndpointConfigName=endpoint_config_name,\n",
    "    ProductionVariants=[\n",
    "        {\n",
    "            \"VariantName\": \"variant1\",\n",
    "            \"ModelName\": model_name,\n",
    "            \"InstanceType\": \"ml.g5.12xlarge\",\n",
    "            \"InitialInstanceCount\": 1,\n",
    "            \"ContainerStartupHealthCheckTimeoutInSeconds\": 2400,\n",
    "        },\n",
    "    ],\n",
    ")\n",
    "endpoint_config_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_endpoint_response = sm_client.create_endpoint(\n",
    "    EndpointName=f\"{endpoint_name}\", EndpointConfigName=endpoint_config_name\n",
    ")\n",
    "print(f\"Created Endpoint: {create_endpoint_response['EndpointArn']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "resp = sm_client.describe_endpoint(EndpointName=endpoint_name)\n",
    "status = resp[\"EndpointStatus\"]\n",
    "print(\"Status: \" + status)\n",
    "\n",
    "while status == \"Creating\":\n",
    "    time.sleep(60)\n",
    "    resp = sm_client.describe_endpoint(EndpointName=endpoint_name)\n",
    "    status = resp[\"EndpointStatus\"]\n",
    "    print(\"Status: \" + status)\n",
    "\n",
    "print(\"Arn: \" + resp[\"EndpointArn\"])\n",
    "print(\"Status: \" + status)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"import argparse\n",
    "            def main(string: str):\n",
    "                print(string)\n",
    "                print(string[::-1])\n",
    "                if __name__ == \"__main__\":\"\"\"\n",
    "\n",
    "params = { \"max_new_tokens\":256, \n",
    "              \"temperature\":0.1}\n",
    "\n",
    "response_model = smr_client.invoke_endpoint(\n",
    "    EndpointName=endpoint_name,\n",
    "    Body=json.dumps(\n",
    "        {\n",
    "            \"inputs\": prompt,\n",
    "            \"parameters\": params\n",
    "        }\n",
    "    ),\n",
    "    ContentType=\"application/json\",\n",
    ")\n",
    "\n",
    "response_model[\"Body\"].read().decode(\"utf8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import boto3\n",
    "from botocore.exceptions import ClientError\n",
    "\n",
    "api_name = \"lmi-codellama-7b-api\"\n",
    "api_stage = \"prod\"\n",
    "api_region = \"us-east-1\"\n",
    "api_lambda_name = \"lmi-codellama-7b-lambda\"\n",
    "api_lambda_role_name = \"lmi-codellama-7b-lambda-role\"\n",
    "api_lambda_policy_name = \"lmi-codellama-7b-lambda-policy\"\n",
    "\n",
    "lambda_client = boto3.client(\"lambda\")\n",
    "iam_client = boto3.client(\"iam\")\n",
    "apigw_client = boto3.client(\"apigateway\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create IAM role for Lambda\n",
    "try:\n",
    "    lambda_role = iam_client.create_role(\n",
    "        RoleName=api_lambda_role_name,\n",
    "        AssumeRolePolicyDocument='{\"Version\": \"2012-10-17\",\"Statement\": [{\"Effect\": \"Allow\",\"Principal\": {\"Service\": \"lambda.amazonaws.com\"},\"Action\": \"sts:AssumeRole\"}]}'\n",
    "    )\n",
    "    print(f\"Created IAM role: {api_lambda_role_name}\")\n",
    "except ClientError as e:\n",
    "    if e.response['Error']['Code'] == 'EntityAlreadyExists':\n",
    "        print(f\"IAM role {api_lambda_role_name} already exists\")\n",
    "    else:\n",
    "        raise\n",
    "\n",
    "# Attach policy to IAM role\n",
    "policy_document = {\n",
    "    \"Version\": \"2012-10-17\",\n",
    "    \"Statement\": [\n",
    "        {\n",
    "            \"Effect\": \"Allow\",\n",
    "            \"Action\": [\n",
    "                \"logs:CreateLogGroup\",\n",
    "                \"logs:CreateLogStream\",\n",
    "                \"logs:PutLogEvents\",\n",
    "                \"sagemaker:InvokeEndpoint\"\n",
    "            ],\n",
    "            \"Resource\": \"*\"\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "try:\n",
    "    iam_client.put_role_policy(\n",
    "        RoleName=api_lambda_role_name,\n",
    "        PolicyName=api_lambda_policy_name,\n",
    "        PolicyDocument=json.dumps(policy_document)\n",
    "    )\n",
    "    print(f\"Attached policy to IAM role: {api_lambda_role_name}\")\n",
    "except ClientError as e:\n",
    "    print(f\"Error attaching policy to IAM role: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the status of the IAM role\n",
    "response = iam_client.get_role(RoleName=api_lambda_role_name)\n",
    "role_arn = response[\"Role\"][\"Arn\"]\n",
    "print(f\"IAM role ARN: {role_arn}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zip the Lambda function\n",
    "!rm -rf lambda_function.zip\n",
    "!zip -r lambda_function.zip lambda_function.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Lambda function\n",
    "zipfile = open(\"lambda_function.zip\", \"rb\").read()\n",
    "\n",
    "try:\n",
    "    lambda_response = lambda_client.create_function(\n",
    "        FunctionName=api_lambda_name,\n",
    "        Runtime='python3.10',\n",
    "        Role=lambda_role['Role']['Arn'],\n",
    "        Handler='lambda_function.lambda_handler',\n",
    "        Code=dict(ZipFile=zipfile),\n",
    "        Timeout=30,\n",
    "        Environment={\n",
    "            'Variables': {\n",
    "                'SAGEMAKER_ENDPOINT_NAME': endpoint_name,\n",
    "                'REGION_NAME': api_region\n",
    "            }\n",
    "        }\n",
    "    )\n",
    "    print(f\"Created Lambda function: {api_lambda_name}\")\n",
    "except ClientError as e:\n",
    "    if e.response['Error']['Code'] == 'ResourceConflictException':\n",
    "        print(f\"Lambda function {api_lambda_name} already exists\")\n",
    "    else:\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the status of the Lambda function\n",
    "response = lambda_client.get_function(FunctionName=api_lambda_name)\n",
    "lambda_arn = response[\"Configuration\"][\"FunctionArn\"]\n",
    "print(f\"Lambda function ARN: {lambda_arn}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create API Gateway\n",
    "try:\n",
    "    api_response = apigw_client.create_rest_api(\n",
    "        name=api_name,\n",
    "        description='API for CodeLlama 7B model',\n",
    "        endpointConfiguration={'types': ['REGIONAL']}\n",
    "    )\n",
    "    api_id = api_response['id']\n",
    "    print(f\"Created API Gateway: {api_name}\")\n",
    "except ClientError as e:\n",
    "    print(f\"Error creating API Gateway: {e}\")\n",
    "    api_id = None\n",
    "\n",
    "if api_id:\n",
    "    # Get API Gateway root resource ID\n",
    "    resources = apigw_client.get_resources(restApiId=api_id)\n",
    "    root_id = [resource for resource in resources['items'] if resource['path'] == '/'][0]['id']\n",
    "\n",
    "    # Create API Gateway method\n",
    "    apigw_client.put_method(\n",
    "        restApiId=api_id,\n",
    "        resourceId=root_id,\n",
    "        httpMethod='POST',\n",
    "        authorizationType='NONE'\n",
    "    )\n",
    "\n",
    "    # Set up API Gateway integration with Lambda\n",
    "    apigw_client.put_integration(\n",
    "        restApiId=api_id,\n",
    "        resourceId=root_id,\n",
    "        httpMethod='POST',\n",
    "        type='AWS_PROXY',\n",
    "        integrationHttpMethod='POST',\n",
    "        uri=f\"arn:aws:apigateway:{api_region}:lambda:path/2015-03-31/functions/{lambda_arn}/invocations\"\n",
    "    )\n",
    "\n",
    "    # Deploy API\n",
    "    apigw_client.create_deployment(\n",
    "        restApiId=api_id,\n",
    "        stageName=api_stage\n",
    "    )\n",
    "\n",
    "    print(f\"API Gateway deployed. Endpoint URL: https://{api_id}.execute-api.{api_region}.amazonaws.com/{api_stage}\")\n",
    "else:\n",
    "    print(\"Failed to create API Gateway. Skipping deployment.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add permission for API Gateway to invoke Lambda\n",
    "try:\n",
    "    lambda_client.add_permission(\n",
    "        FunctionName=lambda_arn,\n",
    "        StatementId=f'apigateway-invoke-{api_id}',\n",
    "        Action='lambda:InvokeFunction',\n",
    "        Principal='apigateway.amazonaws.com',\n",
    "        SourceArn=f\"arn:aws:execute-api:{api_region}:{account_id}:{api_id}/*/*\"\n",
    "    )\n",
    "    print(f\"Added permission for API Gateway to invoke Lambda function\")\n",
    "except ClientError as e:\n",
    "    if e.response['Error']['Code'] != 'ResourceConflictException':\n",
    "        print(f\"Error adding Lambda permission: {e}\")\n",
    "        # You might want to handle this error, possibly by cleaning up created resources\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Invoke the API Gateway using curl and parse the response\n",
    "!curl -X POST -H \"Content-Type: application/json\" -d '{\"prompt\": \"import argparse\\ndef main(string: str):\\n    print(string)\\n    print(string[::-1])\\n    if __name__ == \\\"__main__\\\":\", \"parameters\": {\"max_new_tokens\": 256, \"temperature\": 0.1}}' https://46a8ty7965.execute-api.us-east-1.amazonaws.com/prod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up\n",
    "# Delete the endpoint\n",
    "sm_client.delete_endpoint(EndpointName=endpoint_name)\n",
    "\n",
    "# Delete the endpoint configuration\n",
    "sm_client.delete_endpoint_config(EndpointConfigName=endpoint_config_name)\n",
    "\n",
    "# Delete the model\n",
    "sm_client.delete_model(ModelName=model_name)\n",
    "\n",
    "# Delete the Lambda function\n",
    "lambda_client.delete_function(FunctionName=api_lambda_name)\n",
    "\n",
    "# Detach the policy from the IAM role\n",
    "iam_client.delete_role_policy(\n",
    "    RoleName=api_lambda_role_name,\n",
    "    PolicyName=api_lambda_policy_name\n",
    ")\n",
    "\n",
    "# Delete the IAM role\n",
    "iam_client.delete_role(RoleName=api_lambda_role_name)\n",
    "\n",
    "# Delete the API Gateway\n",
    "apigw_client.delete_rest_api(restApiId=api_id)\n",
    "\n",
    "print(\"Clean up complete\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
